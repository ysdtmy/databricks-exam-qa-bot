# Databricks notebook source
# MAGIC %md
# MAGIC # 01: Databricks å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚¯ãƒ­ãƒ¼ãƒ«
# MAGIC
# MAGIC Databricks å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è©¦é¨“ç¯„å›²ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€
# MAGIC ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

# COMMAND ----------

# MAGIC %pip install langchain-text-splitters beautifulsoup4 requests
# MAGIC %restart_python

# COMMAND ----------

# MAGIC %md
# MAGIC ## è¨­å®š

# COMMAND ----------

# è¨­å®š - ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
CATALOG_NAME = "exam_qa_bot"
SCHEMA_NAME = "default"
TABLE_NAME = "docs_chunks"

FULL_TABLE_NAME = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

# ãƒãƒ£ãƒ³ã‚¯è¨­å®š
CHUNK_SIZE = 500  # æ–‡å­—æ•°
CHUNK_OVERLAP = 100  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ–‡å­—æ•°

# ã‚¯ãƒ­ãƒ¼ãƒ«è¨­å®š
MAX_PAGES_PER_SEED = 20  # å„ã‚·ãƒ¼ãƒ‰ URL ã‹ã‚‰è¾¿ã‚‹ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®æœ€å¤§æ•°
CRAWL_DELAY = 0.5  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒã®ä½œæˆ

# COMMAND ----------

spark.sql(f"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}")
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ URL ã®å®šç¾©
# MAGIC
# MAGIC å„ã‚«ãƒ†ã‚´ãƒªã®ã‚·ãƒ¼ãƒ‰ URL ã‚’èµ·ç‚¹ã«ã€åŒä¸€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚‚
# MAGIC è‡ªå‹•çš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã™ï¼ˆå„ã‚·ãƒ¼ãƒ‰ã‹ã‚‰æœ€å¤§ `MAX_PAGES_PER_SEED` ãƒšãƒ¼ã‚¸ï¼‰ã€‚

# COMMAND ----------

# Data Engineer Associate è©¦é¨“ç¯„å›²ã«æ²¿ã£ãŸã‚·ãƒ¼ãƒ‰ URL
# sitemap.xml ã‹ã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’è‡ªå‹•ç™ºè¦‹ã—ã¾ã™
CRAWL_SEEDS = {
    "Databricks Intelligence Platform": [
        "https://docs.databricks.com/aws/en/getting-started/concepts",
        "https://docs.databricks.com/aws/en/introduction",
        "https://docs.databricks.com/aws/en/compute",
        "https://docs.databricks.com/aws/en/compute/sql-warehouse/",
        "https://docs.databricks.com/aws/en/sql/",
        "https://docs.databricks.com/aws/en/repos/",
        "https://docs.databricks.com/aws/en/notebooks/",
        "https://docs.databricks.com/aws/en/dbfs/",
    ],
    "Development & Ingestion": [
        "https://docs.databricks.com/aws/en/delta/",
        "https://docs.databricks.com/aws/en/delta/merge",
        "https://docs.databricks.com/aws/en/delta/history",
        "https://docs.databricks.com/aws/en/ingestion/auto-loader/",
        "https://docs.databricks.com/aws/en/ingestion/copy-into/",
        "https://docs.databricks.com/aws/en/data-engineering/",
        "https://docs.databricks.com/aws/en/connect/",
    ],
    "Data Processing & Transformations": [
        "https://docs.databricks.com/aws/en/spark/",
        "https://docs.databricks.com/aws/en/pyspark/",
        "https://docs.databricks.com/aws/en/sql/language-manual/",
        "https://docs.databricks.com/aws/en/structured-streaming/",
        "https://docs.databricks.com/aws/en/udf/",
        "https://docs.databricks.com/aws/en/optimizations/",
        "https://docs.databricks.com/aws/en/delta/data-skipping",
    ],
    "Productionizing Data Pipelines": [
        "https://docs.databricks.com/aws/en/delta-live-tables/",
        "https://docs.databricks.com/aws/en/workflows/",
        "https://docs.databricks.com/aws/en/jobs/",
    ],
    "Data Governance & Quality": [
        "https://docs.databricks.com/aws/en/data-governance/unity-catalog/",
        "https://docs.databricks.com/aws/en/data-governance/unity-catalog/manage-privileges/",
        "https://docs.databricks.com/aws/en/tables/constraints",
        "https://docs.databricks.com/aws/en/delta-live-tables/expectations",
        "https://docs.databricks.com/aws/en/data-governance/",
    ],
}

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ« & ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
# MAGIC
# MAGIC LangChain ã® `RecursiveCharacterTextSplitter` ã‚’ä½¿ç”¨ã—ã¦ã€
# MAGIC æ„å‘³ã®ã‚ã‚‹å¢ƒç•Œï¼ˆæ®µè½ãƒ»æ–‡ï¼‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¾ã™ã€‚
# MAGIC ã‚·ãƒ¼ãƒ‰ URL ã‹ã‚‰ãƒªãƒ³ã‚¯å…ˆã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•ç™ºè¦‹ã—ã¦ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã™ã€‚

# COMMAND ----------

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import xml.etree.ElementTree as ET
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã®åˆæœŸåŒ–
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""],
    is_separator_regex=False,
)

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; DatabricksExamBot/1.0)"}
DOCS_DOMAIN = "https://docs.databricks.com"
SITEMAP_URL = "https://docs.databricks.com/sitemap.xml"


def fetch_page(url: str):
    """URL ã‹ã‚‰ãƒšãƒ¼ã‚¸ã® BeautifulSoup ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        print(f"    âš  å–å¾—å¤±æ•—: {url} - {e}")
        return None


def extract_text(soup) -> str:
    """BeautifulSoup ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    # ä¸è¦è¦ç´ ã‚’é™¤å»
    for tag in soup.find_all(["nav", "header", "footer", "script", "style", "aside"]):
        tag.decompose()

    main = soup.find("main") or soup.find("article") or soup.find("div", {"role": "main"})
    text = (main or soup).get_text(separator="\n", strip=True)

    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r" {2,}", " ", text)
    return text.strip()


def fetch_sitemap_urls() -> list[str]:
    """sitemap.xml ã‹ã‚‰å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ URL ã‚’å–å¾—ï¼ˆ1 å›ã ã‘å–å¾—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    try:
        response = requests.get(SITEMAP_URL, headers=HEADERS, timeout=30)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        # sitemap.xml ã® namespace
        ns = {"s": "http://www.sitemaps.org/schemas/sitemap/0.9"}
        urls = [loc.text for loc in root.findall(".//s:loc", ns) if loc.text]
        print(f"ğŸ“¡ sitemap.xml ã‹ã‚‰ {len(urls)} ä»¶ã® URL ã‚’å–å¾—")
        return urls
    except Exception as e:
        print(f"âš  sitemap.xml ã®å–å¾—ã«å¤±æ•—: {e}")
        return []


# sitemap ã‚’ 1 å›ã ã‘å–å¾—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
SITEMAP_URLS = fetch_sitemap_urls()


def discover_links_from_sitemap(seed_url: str) -> list[str]:
    """sitemap.xml ã‹ã‚‰ã‚·ãƒ¼ãƒ‰ URL ã¨åŒä¸€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹

    ã‚·ãƒ¼ãƒ‰ URL ã®ãƒ‘ã‚¹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã«ä¸€è‡´ã™ã‚‹ URL ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
    ä¾‹: seed = .../aws/en/compute â†’ .../aws/en/compute/* ã‚’å…¨ã¦è¿”ã™
    """
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
    parsed = urlparse(seed_url)
    seed_path = parsed.path.rstrip("/")

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
    # ä¾‹: /aws/en/compute/sql-warehouse â†’ /aws/en/compute/sql-warehouse/
    # ä¾‹: /aws/en/delta/merge â†’ /aws/en/delta/
    # ãƒšãƒ¼ã‚¸å€‹åˆ¥ URL ã®å ´åˆã¯è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã™ã‚‹
    section_prefix = seed_path
    if not seed_path.endswith("/"):
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ãƒšãƒ¼ã‚¸ã‹ã‚’åˆ¤å®š
        # sitemap å†…ã« seed_path + "/" ã§å§‹ã¾ã‚‹ URL ãŒã‚ã‚Œã°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        has_children = any(
            urlparse(u).path.startswith(seed_path + "/") for u in SITEMAP_URLS
        )
        if not has_children:
            # ãƒšãƒ¼ã‚¸å€‹åˆ¥ URL â†’ è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã™ã‚‹
            section_prefix = seed_path.rsplit("/", 1)[0]

    section_prefix = section_prefix.rstrip("/") + "/"

    links = []
    seen = set()
    seed_normalized = seed_url.rstrip("/")

    for sitemap_url in SITEMAP_URLS:
        sitemap_path = urlparse(sitemap_url).path
        if not sitemap_path.startswith(section_prefix):
            continue

        url_normalized = sitemap_url.rstrip("/")
        if url_normalized in seen or url_normalized == seed_normalized:
            continue

        seen.add(url_normalized)
        links.append(sitemap_url)

    return links


def discover_links_from_content(soup, seed_url: str) -> list[str]:
    """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®ãƒªãƒ³ã‚¯ã‚‚æŠ½å‡ºï¼ˆsitemap ã®è£œå®Œç”¨ï¼‰"""
    section = ""
    path = urlparse(seed_url).path
    # /aws/en/section/... ã¾ãŸã¯ /en/section/... ã®ä¸¡æ–¹ã«å¯¾å¿œ
    match = re.search(r"/en/([^/]+)", path)
    if match:
        section = match.group(1)

    if not section:
        return []

    links = []
    seen = set()

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    main = soup.find("main") or soup.find("article") or soup
    for a_tag in main.find_all("a", href=True):
        href = a_tag["href"]
        full_url = urljoin(seed_url, href)
        full_url = full_url.split("#")[0].split("?")[0]

        if not full_url.startswith(DOCS_DOMAIN):
            continue

        full_path = urlparse(full_url).path
        if f"/en/{section}" not in full_path:
            continue

        if full_url in seen or full_url == seed_url:
            continue

        seen.add(full_url)
        links.append(full_url)

    return links


def crawl_seed(seed_url: str, max_pages: int) -> list[tuple[str, str]]:
    """ã‚·ãƒ¼ãƒ‰ URL ã¨ãã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€(url, text) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    results = []

    # ã¾ãšã‚·ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    raw_soup = fetch_page(seed_url)
    if raw_soup is None:
        return results

    # sitemap.xml ã‹ã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
    sitemap_links = discover_links_from_sitemap(seed_url)
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ãƒªãƒ³ã‚¯ã§è£œå®Œ
    content_links = discover_links_from_content(raw_soup, seed_url)

    # sitemap å„ªå…ˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§è£œå®Œï¼ˆé‡è¤‡æ’é™¤ï¼‰
    all_links = list(dict.fromkeys(sitemap_links + content_links))
    print(f"    â†’ sitemap: {len(sitemap_links)} ä»¶, ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(content_links)} ä»¶, åˆè¨ˆ: {len(all_links)} ä»¶")

    # ã‚·ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸è‡ªä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    text = extract_text(raw_soup)
    if text and len(text) > 100:
        results.append((seed_url, text))

    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆä¸Šé™ã‚ã‚Šï¼‰
    for sub_url in all_links[:max_pages]:
        time.sleep(CRAWL_DELAY)
        sub_soup = fetch_page(sub_url)
        if sub_soup is None:
            continue
        sub_text = extract_text(sub_soup)
        if sub_text and len(sub_text) > 100:
            results.append((sub_url, sub_text))
            print(f"    ğŸ“„ {sub_url} ({len(sub_text)} æ–‡å­—)")

    return results

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ

# COMMAND ----------

all_chunks = []
crawled_urls = set()  # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡æ’é™¤

import hashlib
from datetime import datetime

crawled_at = datetime.utcnow().isoformat()


def make_chunk_id(source_url: str, chunk_index: int) -> str:
    """URL ã¨ãƒãƒ£ãƒ³ã‚¯ç•ªå·ã‹ã‚‰æ±ºå®šçš„ãª chunk_id ã‚’ç”Ÿæˆï¼ˆå†ªç­‰æ€§ã®æ‹…ä¿ï¼‰"""
    raw = f"{source_url}::{chunk_index}"
    return hashlib.sha256(raw.encode()).hexdigest()[:16]


for category, seed_urls in CRAWL_SEEDS.items():
    print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category}")
    category_page_count = 0
    category_chunk_count = 0

    for seed_url in seed_urls:
        print(f"\n  ğŸ”— ã‚·ãƒ¼ãƒ‰: {seed_url}")
        pages = crawl_seed(seed_url, MAX_PAGES_PER_SEED)

        for url, text in pages:
            if url in crawled_urls:
                continue
            crawled_urls.add(url)

            chunks = text_splitter.split_text(text)
            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    "chunk_id": make_chunk_id(url, i),
                    "category": category,
                    "source_url": url,
                    "content": chunk,
                    "crawled_at": crawled_at,
                })

            category_page_count += 1
            category_chunk_count += len(chunks)

        time.sleep(CRAWL_DELAY)

    print(f"\n  ğŸ“Š {category}: {category_page_count} ãƒšãƒ¼ã‚¸, {category_chunk_count} ãƒãƒ£ãƒ³ã‚¯")

print(f"\n{'='*50}")
print(f"ğŸ“Š åˆè¨ˆ: {len(crawled_urls)} ãƒšãƒ¼ã‚¸, {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ï¼ˆå†ªç­‰ãƒ»å®šæœŸå®Ÿè¡Œå¯¾å¿œï¼‰
# MAGIC
# MAGIC - `chunk_id` ã¯ URL + ãƒãƒ£ãƒ³ã‚¯ç•ªå·ã®ãƒãƒƒã‚·ãƒ¥ã§æ±ºå®šçš„ã«ç”Ÿæˆ
# MAGIC - `MERGE INTO` ã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆupsertï¼‰
# MAGIC - ä½•åº¦å®Ÿè¡Œã—ã¦ã‚‚åŒã˜çµæœã«ãªã‚Šã¾ã™ï¼ˆå†ªç­‰æ€§ï¼‰
# MAGIC - `crawled_at` ã§æœ€çµ‚ã‚¯ãƒ­ãƒ¼ãƒ«æ—¥æ™‚ã‚’è¿½è·¡

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, StringType

schema = StructType([
    StructField("chunk_id", StringType(), False),
    StructField("category", StringType(), False),
    StructField("source_url", StringType(), False),
    StructField("content", StringType(), False),
    StructField("crawled_at", StringType(), False),
])

# ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆï¼ˆCDF æœ‰åŠ¹ï¼‰
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {FULL_TABLE_NAME} (
        chunk_id STRING NOT NULL,
        category STRING NOT NULL,
        source_url STRING NOT NULL,
        content STRING NOT NULL,
        crawled_at STRING NOT NULL
    )
    USING DELTA
    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')
""")

print(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ« {FULL_TABLE_NAME} ã‚’ç¢ºèª/ä½œæˆã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## MERGE INTOï¼ˆUpsertï¼‰

# COMMAND ----------

df_new = spark.createDataFrame(all_chunks, schema=schema)
df_new.createOrReplaceTempView("new_chunks")

# MERGE: chunk_id ãŒä¸€è‡´ã—ãŸã‚‰æ›´æ–°ã€ãªã‘ã‚Œã°æŒ¿å…¥
merge_result = spark.sql(f"""
    MERGE INTO {FULL_TABLE_NAME} AS target
    USING new_chunks AS source
    ON target.chunk_id = source.chunk_id
    WHEN MATCHED THEN UPDATE SET
        target.category = source.category,
        target.source_url = source.source_url,
        target.content = source.content,
        target.crawled_at = source.crawled_at
    WHEN NOT MATCHED THEN INSERT *
""")

# å¤ã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆä»Šå›ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å«ã¾ã‚Œãªã„ã‚‚ã®ï¼‰ã‚’å‰Šé™¤
spark.sql(f"""
    DELETE FROM {FULL_TABLE_NAME}
    WHERE crawled_at < '{crawled_at}'
""")

row_count = spark.sql(f"SELECT COUNT(*) AS cnt FROM {FULL_TABLE_NAME}").first()["cnt"]
print(f"âœ… MERGE å®Œäº†: {FULL_TABLE_NAME} ã« {row_count} è¡Œ")
print(f"ğŸ• ã‚¯ãƒ­ãƒ¼ãƒ«æ—¥æ™‚: {crawled_at}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ç¢ºèª

# COMMAND ----------

display(spark.sql(f"SELECT * FROM {FULL_TABLE_NAME} LIMIT 10"))

# COMMAND ----------

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’ç¢ºèª
display(spark.sql(f"""
    SELECT category, COUNT(*) as chunk_count, MIN(crawled_at) as oldest, MAX(crawled_at) as latest
    FROM {FULL_TABLE_NAME}
    GROUP BY category
    ORDER BY category
"""))
