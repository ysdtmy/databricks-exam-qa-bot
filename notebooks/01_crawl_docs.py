# Databricks notebook source
# MAGIC %md
# MAGIC # 01: Databricks å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚¯ãƒ­ãƒ¼ãƒ«
# MAGIC
# MAGIC Databricks å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è©¦é¨“ç¯„å›²ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€
# MAGIC ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ## è¨­å®š

# COMMAND ----------

# è¨­å®š - ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
CATALOG_NAME = "main"
SCHEMA_NAME = "exam_bot"
TABLE_NAME = "docs_chunks"

FULL_TABLE_NAME = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

# ãƒãƒ£ãƒ³ã‚¯è¨­å®š
CHUNK_SIZE = 500  # æ–‡å­—æ•°
CHUNK_OVERLAP = 100  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ–‡å­—æ•°

# COMMAND ----------

# MAGIC %pip install langchain-text-splitters beautifulsoup4 requests
# MAGIC %restart_python

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒã®ä½œæˆ

# COMMAND ----------

spark.sql(f"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}")
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ URL ã®å®šç¾©

# COMMAND ----------

# Data Engineer Associate è©¦é¨“ç¯„å›²ã«æ²¿ã£ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ URL
CRAWL_URLS = {
    "Databricks Intelligence Platform": [
        "https://docs.databricks.com/en/getting-started/concepts.html",
        "https://docs.databricks.com/en/introduction/index.html",
        "https://docs.databricks.com/en/compute/index.html",
        "https://docs.databricks.com/en/sql/index.html",
        "https://docs.databricks.com/en/repos/index.html",
    ],
    "Development & Ingestion": [
        "https://docs.databricks.com/en/delta/index.html",
        "https://docs.databricks.com/en/ingestion/auto-loader/index.html",
        "https://docs.databricks.com/en/ingestion/copy-into/index.html",
        "https://docs.databricks.com/en/sql/language-manual/delta-create-table.html",
        "https://docs.databricks.com/en/delta/merge.html",
        "https://docs.databricks.com/en/tables/multi-hop.html",
    ],
    "Data Processing & Transformations": [
        "https://docs.databricks.com/en/spark/index.html",
        "https://docs.databricks.com/en/pyspark/index.html",
        "https://docs.databricks.com/en/sql/language-manual/index.html",
        "https://docs.databricks.com/en/structured-streaming/index.html",
        "https://docs.databricks.com/en/udf/index.html",
        "https://docs.databricks.com/en/spark/caching.html",
    ],
    "Productionizing Data Pipelines": [
        "https://docs.databricks.com/en/delta-live-tables/index.html",
        "https://docs.databricks.com/en/workflows/index.html",
        "https://docs.databricks.com/en/workflows/jobs/create-run-jobs.html",
        "https://docs.databricks.com/en/jobs/schedule.html",
    ],
    "Data Governance & Quality": [
        "https://docs.databricks.com/en/data-governance/unity-catalog/index.html",
        "https://docs.databricks.com/en/tables/constraints.html",
        "https://docs.databricks.com/en/delta-live-tables/expectations.html",
        "https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/index.html",
    ],
}

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ« & ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
# MAGIC
# MAGIC LangChain ã® `RecursiveCharacterTextSplitter` ã‚’ä½¿ç”¨ã—ã¦ã€
# MAGIC æ„å‘³ã®ã‚ã‚‹å¢ƒç•Œï¼ˆæ®µè½ãƒ»æ–‡ï¼‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¾ã™ã€‚

# COMMAND ----------

import requests
from bs4 import BeautifulSoup
import re
import time
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã®åˆæœŸåŒ–
# ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®å„ªå…ˆé †ä½: æ®µè½ â†’ æ”¹è¡Œ â†’ å¥ç‚¹ â†’ ãƒ”ãƒªã‚ªãƒ‰ â†’ ã‚¹ãƒšãƒ¼ã‚¹ â†’ æ–‡å­—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""],
    is_separator_regex=False,
)


def fetch_page_text(url: str) -> str:
    """URL ã‹ã‚‰ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; DatabricksExamBot/1.0)"
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤å»
        for tag in soup.find_all(["nav", "header", "footer", "script", "style", "aside"]):
            tag.decompose()

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é ˜åŸŸã‚’å–å¾—
        main = soup.find("main") or soup.find("article") or soup.find("div", {"role": "main"})
        if main:
            text = main.get_text(separator="\n", strip=True)
        else:
            text = soup.get_text(separator="\n", strip=True)

        # é€£ç¶šæ”¹è¡Œãƒ»ç©ºç™½ã‚’æ•´ç†
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = re.sub(r" {2,}", " ", text)
        return text.strip()
    except Exception as e:
        print(f"  âš  ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—: {url} - {e}")
        return ""

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ

# COMMAND ----------

all_chunks = []
chunk_id = 0

for category, urls in CRAWL_URLS.items():
    print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category}")
    for url in urls:
        print(f"  ğŸ”— ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {url}")
        text = fetch_page_text(url)
        if not text:
            continue

        chunks = text_splitter.split_text(text)
        print(f"  âœ… {len(chunks)} ãƒãƒ£ãƒ³ã‚¯å–å¾—")

        for chunk in chunks:
            all_chunks.append({
                "chunk_id": chunk_id,
                "category": category,
                "source_url": url,
                "content": chunk,
            })
            chunk_id += 1

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(1)

print(f"\nğŸ“Š åˆè¨ˆ: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("chunk_id", IntegerType(), False),
    StructField("category", StringType(), False),
    StructField("source_url", StringType(), False),
    StructField("content", StringType(), False),
])

df = spark.createDataFrame(all_chunks, schema=schema)

# Change Data Feed ã‚’æœ‰åŠ¹ã«ã—ã¦ä¿å­˜ï¼ˆVector Search ã® Delta Sync ã«å¿…è¦ï¼‰
df.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .option("delta.enableChangeDataFeed", "true") \
    .saveAsTable(FULL_TABLE_NAME)

print(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ« {FULL_TABLE_NAME} ã« {df.count()} è¡Œã‚’ä¿å­˜ã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ç¢ºèª

# COMMAND ----------

display(spark.sql(f"SELECT * FROM {FULL_TABLE_NAME} LIMIT 10"))

# COMMAND ----------

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’ç¢ºèª
display(spark.sql(f"""
    SELECT category, COUNT(*) as chunk_count
    FROM {FULL_TABLE_NAME}
    GROUP BY category
    ORDER BY category
"""))
