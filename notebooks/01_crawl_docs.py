# Databricks notebook source
# MAGIC %md
# MAGIC # 01: Databricks å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚¯ãƒ­ãƒ¼ãƒ«
# MAGIC
# MAGIC Databricks å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è©¦é¨“ç¯„å›²ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€
# MAGIC ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

# COMMAND ----------

# MAGIC %pip install langchain-text-splitters beautifulsoup4 requests
# MAGIC %restart_python

# COMMAND ----------

# MAGIC %md
# MAGIC ## è¨­å®š

# COMMAND ----------

# è¨­å®š - ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
CATALOG_NAME = "exam_qa_bot"
SCHEMA_NAME = "default"
TABLE_NAME = "docs_chunks"

FULL_TABLE_NAME = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

# ãƒãƒ£ãƒ³ã‚¯è¨­å®š
CHUNK_SIZE = 500  # æ–‡å­—æ•°
CHUNK_OVERLAP = 100  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ–‡å­—æ•°

# ã‚¯ãƒ­ãƒ¼ãƒ«è¨­å®š
MAX_PAGES_PER_SEED = 20  # å„ã‚·ãƒ¼ãƒ‰ URL ã‹ã‚‰è¾¿ã‚‹ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®æœ€å¤§æ•°
CRAWL_DELAY = 0.5  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒã®ä½œæˆ

# COMMAND ----------

spark.sql(f"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}")
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ URL ã®å®šç¾©
# MAGIC
# MAGIC å„ã‚«ãƒ†ã‚´ãƒªã®ã‚·ãƒ¼ãƒ‰ URL ã‚’èµ·ç‚¹ã«ã€åŒä¸€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚‚
# MAGIC è‡ªå‹•çš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã™ï¼ˆå„ã‚·ãƒ¼ãƒ‰ã‹ã‚‰æœ€å¤§ `MAX_PAGES_PER_SEED` ãƒšãƒ¼ã‚¸ï¼‰ã€‚

# COMMAND ----------

# Data Engineer Associate è©¦é¨“ç¯„å›²ã«æ²¿ã£ãŸã‚·ãƒ¼ãƒ‰ URL
# index ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹è©³ç´°ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•ã‚¯ãƒ­ãƒ¼ãƒ«ã•ã‚Œã¾ã™
CRAWL_SEEDS = {
    "Databricks Intelligence Platform": [
        "https://docs.databricks.com/en/getting-started/concepts.html",
        "https://docs.databricks.com/en/introduction/index.html",
        "https://docs.databricks.com/en/compute/index.html",
        "https://docs.databricks.com/en/compute/sql-warehouse/index.html",
        "https://docs.databricks.com/en/sql/index.html",
        "https://docs.databricks.com/en/repos/index.html",
        "https://docs.databricks.com/en/notebooks/index.html",
        "https://docs.databricks.com/en/dbfs/index.html",
    ],
    "Development & Ingestion": [
        "https://docs.databricks.com/en/delta/index.html",
        "https://docs.databricks.com/en/delta/create-tables.html",
        "https://docs.databricks.com/en/delta/merge.html",
        "https://docs.databricks.com/en/delta/update.html",
        "https://docs.databricks.com/en/delta/delete-on.html",
        "https://docs.databricks.com/en/delta/history.html",
        "https://docs.databricks.com/en/delta/time-travel.html",
        "https://docs.databricks.com/en/ingestion/auto-loader/index.html",
        "https://docs.databricks.com/en/ingestion/copy-into/index.html",
        "https://docs.databricks.com/en/tables/multi-hop.html",
        "https://docs.databricks.com/en/connect/external-systems/index.html",
    ],
    "Data Processing & Transformations": [
        "https://docs.databricks.com/en/spark/index.html",
        "https://docs.databricks.com/en/pyspark/index.html",
        "https://docs.databricks.com/en/pyspark/basics.html",
        "https://docs.databricks.com/en/sql/language-manual/index.html",
        "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select.html",
        "https://docs.databricks.com/en/structured-streaming/index.html",
        "https://docs.databricks.com/en/structured-streaming/triggers.html",
        "https://docs.databricks.com/en/structured-streaming/watermarks.html",
        "https://docs.databricks.com/en/udf/index.html",
        "https://docs.databricks.com/en/spark/caching.html",
        "https://docs.databricks.com/en/optimizations/index.html",
        "https://docs.databricks.com/en/delta/data-skipping.html",
    ],
    "Productionizing Data Pipelines": [
        "https://docs.databricks.com/en/delta-live-tables/index.html",
        "https://docs.databricks.com/en/delta-live-tables/tutorial.html",
        "https://docs.databricks.com/en/delta-live-tables/updates.html",
        "https://docs.databricks.com/en/delta-live-tables/observability.html",
        "https://docs.databricks.com/en/workflows/index.html",
        "https://docs.databricks.com/en/workflows/jobs/create-run-jobs.html",
        "https://docs.databricks.com/en/workflows/jobs/schedule-jobs.html",
        "https://docs.databricks.com/en/workflows/jobs/monitor-jobs.html",
        "https://docs.databricks.com/en/jobs/index.html",
        "https://docs.databricks.com/en/jobs/schedule.html",
    ],
    "Data Governance & Quality": [
        "https://docs.databricks.com/en/data-governance/unity-catalog/index.html",
        "https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/index.html",
        "https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html",
        "https://docs.databricks.com/en/data-governance/unity-catalog/create-schemas.html",
        "https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html",
        "https://docs.databricks.com/en/tables/constraints.html",
        "https://docs.databricks.com/en/delta-live-tables/expectations.html",
        "https://docs.databricks.com/en/data-governance/index.html",
    ],
}

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ« & ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
# MAGIC
# MAGIC LangChain ã® `RecursiveCharacterTextSplitter` ã‚’ä½¿ç”¨ã—ã¦ã€
# MAGIC æ„å‘³ã®ã‚ã‚‹å¢ƒç•Œï¼ˆæ®µè½ãƒ»æ–‡ï¼‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¾ã™ã€‚
# MAGIC ã‚·ãƒ¼ãƒ‰ URL ã‹ã‚‰ãƒªãƒ³ã‚¯å…ˆã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚‚è‡ªå‹•ç™ºè¦‹ã—ã¦ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã™ã€‚

# COMMAND ----------

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã®åˆæœŸåŒ–
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""],
    is_separator_regex=False,
)

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; DatabricksExamBot/1.0)"}
DOCS_DOMAIN = "https://docs.databricks.com"


def fetch_page(url: str):
    """URL ã‹ã‚‰ãƒšãƒ¼ã‚¸ã® BeautifulSoup ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        print(f"    âš  å–å¾—å¤±æ•—: {url} - {e}")
        return None


def extract_text(soup) -> str:
    """BeautifulSoup ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    # ä¸è¦è¦ç´ ã‚’é™¤å»
    for tag in soup.find_all(["nav", "header", "footer", "script", "style", "aside"]):
        tag.decompose()

    main = soup.find("main") or soup.find("article") or soup.find("div", {"role": "main"})
    text = (main or soup).get_text(separator="\n", strip=True)

    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r" {2,}", " ", text)
    return text.strip()


def _extract_section(url: str) -> str:
    """URL ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã‚’æŠ½å‡º
    ä¾‹: /en/compute/index.html â†’ 'compute'
        /aws/en/delta/merge.html â†’ 'delta'
    """
    path = urlparse(url).path
    # /en/ ã¾ãŸã¯ /<cloud>/en/ ã®å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    match = re.search(r"/en/([^/]+)", path)
    return match.group(1) if match else ""


def discover_links(soup, seed_url: str) -> list[str]:
    """ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰ Databricks ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹

    Databricks ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ãƒªãƒ³ã‚¯ã« /aws/en/, /gcp/en/, /azure/en/
    ãªã©ã®ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã§ç…§åˆã™ã‚‹ã€‚
    """
    section = _extract_section(seed_url)
    if not section:
        return []

    links = []
    seen = set()

    # ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆ<main> å†…ã ã‘ã§ãªãï¼‰
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        full_url = urljoin(seed_url, href)

        # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã¨ã‚¯ã‚¨ãƒªã‚’é™¤å»
        full_url = full_url.split("#")[0].split("?")[0]

        # Databricks ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã«é™å®š
        if not full_url.startswith(DOCS_DOMAIN):
            continue

        # åŒã˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å±ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ/en/<section>/ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        full_path = urlparse(full_url).path
        if f"/en/{section}" not in full_path:
            continue

        # HTML ãƒšãƒ¼ã‚¸ã®ã¿
        if not (full_path.endswith(".html") or full_path.endswith("/")):
            continue

        # é‡è¤‡æ’é™¤ãƒ»è‡ªåˆ†è‡ªèº«æ’é™¤
        if full_url in seen or full_url == seed_url:
            continue

        seen.add(full_url)
        links.append(full_url)

    return links


def crawl_seed(seed_url: str, max_pages: int) -> list[tuple[str, str]]:
    """ã‚·ãƒ¼ãƒ‰ URL ã¨ãã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€(url, text) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    results = []

    # ã¾ãšã‚·ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    soup = fetch_page(seed_url)
    if soup is None:
        return results

    text = extract_text(soup)
    if text and len(text) > 100:
        results.append((seed_url, text))

    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
    sub_links = discover_links(soup, seed_url)
    print(f"    â†’ {len(sub_links)} ä»¶ã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹")

    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆä¸Šé™ã‚ã‚Šï¼‰
    for sub_url in sub_links[:max_pages]:
        time.sleep(CRAWL_DELAY)
        sub_soup = fetch_page(sub_url)
        if sub_soup is None:
            continue
        sub_text = extract_text(sub_soup)
        if sub_text and len(sub_text) > 100:
            results.append((sub_url, sub_text))
            print(f"    ğŸ“„ {sub_url} ({len(sub_text)} æ–‡å­—)")

    return results

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ

# COMMAND ----------

all_chunks = []
crawled_urls = set()  # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡æ’é™¤

import hashlib
from datetime import datetime

crawled_at = datetime.utcnow().isoformat()


def make_chunk_id(source_url: str, chunk_index: int) -> str:
    """URL ã¨ãƒãƒ£ãƒ³ã‚¯ç•ªå·ã‹ã‚‰æ±ºå®šçš„ãª chunk_id ã‚’ç”Ÿæˆï¼ˆå†ªç­‰æ€§ã®æ‹…ä¿ï¼‰"""
    raw = f"{source_url}::{chunk_index}"
    return hashlib.sha256(raw.encode()).hexdigest()[:16]


for category, seed_urls in CRAWL_SEEDS.items():
    print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category}")
    category_page_count = 0
    category_chunk_count = 0

    for seed_url in seed_urls:
        print(f"\n  ğŸ”— ã‚·ãƒ¼ãƒ‰: {seed_url}")
        pages = crawl_seed(seed_url, MAX_PAGES_PER_SEED)

        for url, text in pages:
            if url in crawled_urls:
                continue
            crawled_urls.add(url)

            chunks = text_splitter.split_text(text)
            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    "chunk_id": make_chunk_id(url, i),
                    "category": category,
                    "source_url": url,
                    "content": chunk,
                    "crawled_at": crawled_at,
                })

            category_page_count += 1
            category_chunk_count += len(chunks)

        time.sleep(CRAWL_DELAY)

    print(f"\n  ğŸ“Š {category}: {category_page_count} ãƒšãƒ¼ã‚¸, {category_chunk_count} ãƒãƒ£ãƒ³ã‚¯")

print(f"\n{'='*50}")
print(f"ğŸ“Š åˆè¨ˆ: {len(crawled_urls)} ãƒšãƒ¼ã‚¸, {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Delta ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ï¼ˆå†ªç­‰ãƒ»å®šæœŸå®Ÿè¡Œå¯¾å¿œï¼‰
# MAGIC
# MAGIC - `chunk_id` ã¯ URL + ãƒãƒ£ãƒ³ã‚¯ç•ªå·ã®ãƒãƒƒã‚·ãƒ¥ã§æ±ºå®šçš„ã«ç”Ÿæˆ
# MAGIC - `MERGE INTO` ã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆupsertï¼‰
# MAGIC - ä½•åº¦å®Ÿè¡Œã—ã¦ã‚‚åŒã˜çµæœã«ãªã‚Šã¾ã™ï¼ˆå†ªç­‰æ€§ï¼‰
# MAGIC - `crawled_at` ã§æœ€çµ‚ã‚¯ãƒ­ãƒ¼ãƒ«æ—¥æ™‚ã‚’è¿½è·¡

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, StringType

schema = StructType([
    StructField("chunk_id", StringType(), False),
    StructField("category", StringType(), False),
    StructField("source_url", StringType(), False),
    StructField("content", StringType(), False),
    StructField("crawled_at", StringType(), False),
])

# ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆï¼ˆCDF æœ‰åŠ¹ï¼‰
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {FULL_TABLE_NAME} (
        chunk_id STRING NOT NULL,
        category STRING NOT NULL,
        source_url STRING NOT NULL,
        content STRING NOT NULL,
        crawled_at STRING NOT NULL
    )
    USING DELTA
    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')
""")

print(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ« {FULL_TABLE_NAME} ã‚’ç¢ºèª/ä½œæˆã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## MERGE INTOï¼ˆUpsertï¼‰

# COMMAND ----------

df_new = spark.createDataFrame(all_chunks, schema=schema)
df_new.createOrReplaceTempView("new_chunks")

# MERGE: chunk_id ãŒä¸€è‡´ã—ãŸã‚‰æ›´æ–°ã€ãªã‘ã‚Œã°æŒ¿å…¥
merge_result = spark.sql(f"""
    MERGE INTO {FULL_TABLE_NAME} AS target
    USING new_chunks AS source
    ON target.chunk_id = source.chunk_id
    WHEN MATCHED THEN UPDATE SET
        target.category = source.category,
        target.source_url = source.source_url,
        target.content = source.content,
        target.crawled_at = source.crawled_at
    WHEN NOT MATCHED THEN INSERT *
""")

# å¤ã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆä»Šå›ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«å«ã¾ã‚Œãªã„ã‚‚ã®ï¼‰ã‚’å‰Šé™¤
spark.sql(f"""
    DELETE FROM {FULL_TABLE_NAME}
    WHERE crawled_at < '{crawled_at}'
""")

row_count = spark.sql(f"SELECT COUNT(*) AS cnt FROM {FULL_TABLE_NAME}").first()["cnt"]
print(f"âœ… MERGE å®Œäº†: {FULL_TABLE_NAME} ã« {row_count} è¡Œ")
print(f"ğŸ• ã‚¯ãƒ­ãƒ¼ãƒ«æ—¥æ™‚: {crawled_at}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ç¢ºèª

# COMMAND ----------

display(spark.sql(f"SELECT * FROM {FULL_TABLE_NAME} LIMIT 10"))

# COMMAND ----------

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’ç¢ºèª
display(spark.sql(f"""
    SELECT category, COUNT(*) as chunk_count, MIN(crawled_at) as oldest, MAX(crawled_at) as latest
    FROM {FULL_TABLE_NAME}
    GROUP BY category
    ORDER BY category
"""))
